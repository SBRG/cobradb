#!/usr/bin/env python3

# configure the logger before imports so other packages do not override this
# setup
import logging
import time
import sys
import six

def configure_logger(log_file=None, level=logging.INFO, overwrite_log=True,
                     format=logging.BASIC_FORMAT):
    # console and file
    if log_file is None:
        logging.basicConfig(stream=sys.stdout, level=level, format=format)
    else:
        logging.basicConfig(filename=log_file, level=level,
                            filemode=('w' if overwrite_log else 'a'),
                            format=format)
        console = logging.StreamHandler()
        console.setLevel(level)
        console.setFormatter(logging.Formatter(format))
        logging.getLogger("").addHandler(console)
configure_logger('%s COBRAdb load_db.log' % time.strftime('%Y-%m-%d %H:%M:%S'),
                 level=logging.INFO)


from cobradb.models import *
from cobradb import settings, util
from cobradb.component_loading import get_genbank_accessions, load_genome
from cobradb.model_loading import load_model
from cobradb.map_loading import load_maps_from_server
from cobradb.version_loading import load_version_date

import os
from os import listdir
from os.path import join, isfile
import argparse
from collections import defaultdict


parser = argparse.ArgumentParser()
parser.add_argument('--drop-all', help='Empty database and reload data. NOTE: Does not drop types (e.g. enum categories)', action='store_true')
parser.add_argument('--drop-models', help='Empty model and map data', action='store_true')
parser.add_argument('--drop-maps', help='Empty map data', action='store_true')
parser.add_argument('--skip-genomes', help='Skip genome loading', action='store_true')
parser.add_argument('--skip-models', help='Skip model loading', action='store_true')
parser.add_argument('--skip-maps', help='Skip map loading', action='store_true')

args = parser.parse_args()


def drop_all_tables(engine, enums_to_drop=None):
    """Drops all tables and, optionally, user enums from a postgres database.

    Adapted from: http://www.siafoo.net/snippet/85


    NOTE: To list all the user defined enums, use something like this:

        SELECT DISTINCT t.typname
        FROM pg_catalog.pg_type t
        JOIN pg_catalog.pg_enum e ON t.oid = e.enumtypid;

    """

    from sqlalchemy.sql.expression import text

    table_sql = ("SELECT table_name FROM information_schema.tables "
                 "WHERE table_schema='public' AND table_name NOT LIKE 'pg_%%'")

    for table in [name for (name, ) in engine.execute(text(table_sql))]:
        engine.execute(text('DROP TABLE %s CASCADE' % table))

    # drop the enum types
    if enums_to_drop is not None:
        for enum in enums_to_drop:
            engine.execute(text('DROP TYPE IF EXISTS %s CASCADE' % enum))


if __name__ == '__main__':
    if args.drop_all:
        logging.info('Dropping everything from the database')
        drop_all_tables(engine, custom_enums.keys())

    logging.info('Building the database models')
    Base.metadata.create_all()

    if args.drop_models:
        logging.info('Dropping rows from models')
        connection = engine.connect()
        trans = connection.begin()
        try:
            connection.execute('TRUNCATE model, reaction, component, compartment, deprecated_id CASCADE;')
            trans.commit()
        except:
            trans.rollback()

    # make the session
    session = Session()

    # load the date
    load_version_date(session)

    # get the models and genomes from the model_genome file
    model_dir = settings.model_directory
    model_genome_path = settings.model_genome
    logging.info('Loading models and genomes using %s' % model_genome_path)
    lines = util.load_tsv(model_genome_path, required_column_num=3)
    models_list = []

    def _check_for_additional_gb_filenames(ref_string_plus):
        """Get the extra genbank files from a genome reference."""
        spl = [x.strip() for x in ref_string_plus.split(',')]
        if len(spl) == 1:
            return spl[0], None
        else:
            return spl[0], spl[1:]

    # parse the model_genome file
    for line in lines:
        model_filename, pub_ref_string, genome_ref_string_plus = line
        if genome_ref_string_plus is None:
            genome_ref = None
            additional_gb_filenames = None
        else:
            genome_ref_string, additional_gb_filenames = _check_for_additional_gb_filenames(genome_ref_string_plus)
            genome_ref = util.ref_str_to_tuple(genome_ref_string)
        if pub_ref_string is None or pub_ref_string.strip() == 'None':
            pub_ref = None
        else:
            pub_ref = util.ref_str_to_tuple(pub_ref_string)
        models_list.append({'model_filename': model_filename,
                            'pub_ref': pub_ref,
                            'genome_ref': genome_ref,
                            'additional_gb_filenames': additional_gb_filenames})

    # load the genome
    genomes_for_models = {}
    if not args.skip_genomes:
        # find the accessions for the genbank files
        logging.info('Finding GenBank files')
        refseq_dir = settings.refseq_directory

        # unique refs and additional files for accessions. Any conflicting
        # repeats will raise exception.
        genome_refs = set()
        additional_gb_filenames_dict = defaultdict(set)
        genome_ref_additions = {}
        for d in models_list:
            genome_ref = d['genome_ref']
            if genome_ref is None or genome_ref[0] == 'organism':
                continue
            genome_refs.add(genome_ref)
            additional_gb_filenames = d['additional_gb_filenames']
            if additional_gb_filenames is None:
                continue
            if genome_ref in genome_ref_additions and set(genome_ref_additions[genome_ref]) != set(additional_gb_filenames):
                raise Exception('Conflicting additional files genome ref with %s %s' % genome_ref)
            # Allow for easy lookup of genome refs based on genbank filename
            for add in additional_gb_filenames:
                additional_gb_filenames_dict[add + '.gb'].add(genome_ref)
            genome_ref_additions[genome_ref] = additional_gb_filenames

        # loop through all the files
        genome_file_locations = defaultdict(list)
        for refseq_filename in listdir(refseq_dir):
            refseq_filepath = join(refseq_dir, refseq_filename)
            if refseq_filename.startswith('.') or not isfile(refseq_filepath):
                continue
            # check both accession and assembly for a match
            ids = get_genbank_accessions(refseq_filepath, fast=True)
            # if the ids couldn't be found
            if all(x is None for x in ids.values()):
                logging.warn('Could not find accessions for genbank file %s' % refseq_filepath)
                continue
            # look for matching ids from the model-genome file
            found = False
            for genome_ref in six.iteritems(ids):
                # Ignore assembly and bioproject and force users to provide all
                # genbank files by accession
                if genome_ref[0] != 'ncbi_accession':
                    continue
                if genome_ref in genome_refs:
                    genome_file_locations[genome_ref].append(refseq_filepath)
                    found = True
            # Also look for additional files
            if refseq_filename in additional_gb_filenames_dict:
                for genome_ref in additional_gb_filenames_dict[refseq_filename]:
                    genome_file_locations[genome_ref].append(refseq_filepath)
                found = True
            # warn about unused files
            if not found:
                logging.warn('Unused file in the refseq directory: %s' % refseq_filename)

        # load the genomes
        n = len(genome_refs)
        for i, genome_ref in enumerate(genome_refs):
            logging.info('Loading genome ({} of {}) with {} {}'
                         .format(i + 1, n, genome_ref[0], genome_ref[1]))
            file_paths = genome_file_locations[genome_ref]
            try:
                load_genome(genome_ref, file_paths, session)
            except AlreadyLoadedError as e:
                logging.info(str(e))
            except Exception as e:
                logging.exception(e)


    if not args.skip_models:
        logging.info('Loading models')
        n = len(models_list)
        model_dir = settings.model_directory
        for i, model_dict in enumerate(models_list):
            logging.info('Loading model ({} of {}) {}'
                         .format(i + 1, n, model_dict['model_filename']))
            try:
                load_model(join(model_dir, model_dict['model_filename']),
                           model_dict['pub_ref'],
                           model_dict['genome_ref'],
                           session)
            except AlreadyLoadedError as e:
                logging.info(str(e))
            except Exception as e:
                logging.error('Could not load model %s.' % model_dict['model_filename'])
                logging.exception(e)

    if not args.skip_maps:
        logging.info("Loading Escher maps")
        load_maps_from_server(session, drop_maps=(args.drop_models or args.drop_maps))

    session.close()
    Session.close_all()
